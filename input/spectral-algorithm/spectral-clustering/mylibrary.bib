% This file was created with JabRef 2.9.2.
% Encoding: UTF-8

@ARTICLE{arora2009expander,
  author = {Arora, S. and Rao, S. and Vazirani, U.},
  title = {Expander flows, geometric embeddings and graph partitioning},
  journal = {Journal of the ACM (JACM)},
  year = {2009},
  volume = {56},
  pages = {5},
  number = {2},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {1},
  abstract = {using hyperplanes to separate embedded points?},
  file = {:arora2009-Expander-flows-geometric-embeddings-and-graph-partitioning.pdf:PDF},
  owner = {hpl},
  publisher = {ACM},
  timestamp = {2012.05.24}
}

@BOOK{bach2003learning,
  title = {Learning spectral clustering},
  publisher = {Computer Science Division, University of California},
  year = {2003},
  author = {Bach, F.R. and Jordan, M.I.},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {3},
  file = {:bach2003-Learning-spectral-clustering.pdf:PDF},
  owner = {hpl},
  timestamp = {2012.02.11}
}

@INPROCEEDINGS{brand2003unifying,
  author = {Brand, M. and Huang, K.},
  title = {A unifying theorem for spectral embedding and clustering},
  booktitle = {Proceedings of the Ninth International Workshop on Artificial Intelligence
	and Statistics},
  year = {2003},
  _h_pass1 = {2012.04.30},
  _h_pass2 = {2012.04.30},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {Polarization theorem. 
	
	This may be the root of "projecting to unit sphere" version of SC.},
  file = {:brand2003-A-unifying-theorem-for-spectral-embedding-and-clustering.pdf:PDF},
  owner = {hpl},
  review = {Method:
	
	 1. Eigen decomposition of affinity matrix A. (adjacency similarity
	matrix in graph language).
	
	 2. X = \Lambda ^ 0.5 \tran{V} is the embedding. 
	
	 3. Y is X scaled to unit sphere. (only angles between points are
	concerned)
	
	
	Main result:
	
	 * When less eigen vectors are used, high correlated points are less
	distorted. 
	
	 * With less and less eigen vectors(keep principal ones): angle between
	high affinity points decrease, and xxx low xxx increase. (cluster
	structure)},
  timestamp = {2012.05.02}
}

@INPROCEEDINGS{chen2010isoperimetric,
  author = {Chen, M. and Liu, M. and Liu, J. and Tang, X.},
  title = {Isoperimetric cut on a directed graph},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference
	on},
  year = {2010},
  pages = {2109--2116},
  organization = {IEEE},
  _h_pass1 = {2012.04.04},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {3},
  abstract = {Random Walk Isoperimetric Cut.
	
	
	View spectral clustering as density estimation. Utilize the local
	distribution information. 
	
	
	Intro2.4, the Bayes error view of spectral clustering is another good
	interpretation.},
  file = {:chen2010-Isoperimetric-cut-on-a-directed-graph.pdf:PDF},
  owner = {hpl},
  timestamp = {2012.02.17}
}

@ARTICLE{chung2013local,
  author = {Chung, Fan and Kempton, Mark},
  title = {A local clustering algorithm for connection graphs},
  year = {2013},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  abstract = {prompt by Google Scholar. Looks recent. From Fan Chung.},
  file = {:chung2013-A-local-clustering-algorithm-for-connection-graphs.pdf:PDF},
  owner = {hpl},
  timestamp = {2013.10.09}
}

@TECHREPORT{dhillon2004unified,
  author = {Dhillon, I. and Guan, Y. and Kulis, B.},
  title = {A unified view of kernel k-means, spectral clustering and graph cuts},
  institution = {Univ. of Texas at Austin},
  year = {2004},
  _h_pass1 = {2012.05.15},
  _h_pass2 = {2012.05.15},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {Nice conclusion.
	
	
	Main result:
	
	 * Objective of kernel k-means and some graph combinatoric objectives
	are equal. 
	
	 * The relaxation of those objectives can all result in trace maximization
	problem. 
	
	 * Propose weighted cut, a generalization of RCut and NCut.
	
	 * Can invoke kernel k-means directly without explicit graph construction.(avoid
	computationally intensive EVD)
	
	 * Can use spectral clustering/embedding to initialize kernel k-means.
	
	
	Remarks:
	
	 * Both kernel k-means and the relaxed EVD problem can fall into local
	minima. 
	
	 * Reveal the mathematical foundation: trace maximization. 
	
	 * Neural network also has the same objective. 
	
	 * Thus, DR of ML, ANN, SC/SE, kernel Kmeans, can be used interchangeably
	depending on application requirement.},
  file = {:dhillon2004-A-unified-view-of-kernel-k-means-spectral-clustering-and-graph-cuts.pdf:PDF},
  owner = {hpl},
  publisher = {Computer Science Dept., Univ. of Texas at Austin},
  timestamp = {2012.02.11}
}

@ARTICLE{guattery1998quality,
  author = {Guattery, S. and Miller, G.L.},
  title = {On the quality of spectral separators},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  year = {1998},
  volume = {19},
  pages = {701--719},
  number = {3},
  _h_pass1 = {2012.05.21},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {1},
  abstract = {Theoretical analysis of spectral clustering algorithms. 
	
	(any successors?)
	
	
	Main result:
	
	 * Using more eigenvectors may not help.
	
	 * Some counter examples.(Fig5.1 the Roach Graph)},
  file = {:guattery1998-On-the-quality-of-spectral-separators.pdf:PDF},
  owner = {hpl},
  publisher = {[Philadelphia, Pa.: The Society, c1988-},
  timestamp = {2012.02.11}
}

@ARTICLE{hadley1992efficient,
  author = {Hadley, S.W. and Mark, B.L. and Vannelli, A.},
  title = {An efficient eigenvector approach for finding netlist partitions},
  journal = {Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions
	on},
  year = {1992},
  volume = {11},
  pages = {885--892},
  number = {7},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  abstract = {Spectral Clustering prototype in the field of electronics.},
  file = {:hadley1992-An-efficient-eigenvector-approach-for-finding-netlist-partitions.pdf:PDF},
  owner = {hpl},
  publisher = {IEEE},
  timestamp = {2012.05.02}
}

@ARTICLE{hendrickson1993multidimensional,
  author = {Hendrickson, B. and Leland, R.},
  title = {Multidimensional spectral load balancing},
  journal = {Report SAND93-0074, Sandia National Laboratories, Albuquerque, NM},
  year = {1993},
  volume = {<<missing>>},
  pages = {<<missing>>},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  abstract = {Spectral Clustering prototype in load balancing.},
  file = {:hendrickson1993-Multidimensional-spectral-load-balancing.pdf:PDF},
  owner = {hpl},
  timestamp = {2012.05.02}
}

@ARTICLE{kannan2004clusterings,
  author = {Kannan, R. and Vempala, S. and Vetta, A.},
  title = {On clusterings: Good, bad and spectral},
  journal = {Journal of the ACM (JACM)},
  year = {2004},
  volume = {51},
  pages = {497--515},
  number = {3},
  _h_pass1 = {2012.04.09},
  _h_pass2 = {2012.04.09},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {Propose bicriteria to evaluate clustering. (general clustering, not
	only for spectral). 
	
	
	General criticism:
	
	 * Justification of practitioners is in a case by case manner. 
	
	 * Theoretists criteria are often easy to fool(existence of special
	worst case).
	
	
	Past criteria:(see ch2 for reference)
	
	 * minimum diameter
	
	 * k-center
	
	 * k-median
	
	 * minimum sum},
  file = {:kannan2004-On-clusterings-Good-bad-and-spectral.pdf:PDF},
  owner = {hpl},
  publisher = {ACM},
  review = {General view of spectral clustering:
	
	 * a matrix A. 
	
	 * project the rows into a lower dimension space. 
	
	 * assign cluster in the lower dimension space. 
	
	
	Bicriteria:
	
	 * maximize: the conductance within each cluster. (alpha)
	
	 * minimize: the inter-cluster weights. (epsilon)
	
	
	Remarks:
	
	 * If only alpha is imposed, the "best" clustering will be all singletons.
	
	 * Imposing epsilon, instead of number of clusters(k), has a similar
	funciton of auto model selection.
	
	 * The output is Pareto Efficient boundary of (alpha, spsilon), not
	final decision. 
	
	
	Algorithm1:(top-down)(not only spectral)
	
	 * Find a cut that approximates min conductance cut(any subroutine
	with parameter, alpha, epsilon) in G. 
	
	 * Recurse on resulting subgraphs.
	
	 * (This framework induce bounded alpha and epsilon)
	
	 * If a "good" clustering exists, the bound can be improved.
	
	
	Algorithm2:(top-down)(specialized for spectral)
	
	 * The approximation routine in algo1. is substituted by "ratio cut"
	of 2nd eigenvector v. 
	
	 * "ratio cut": compute each row's projection onto v; set threshold
	of the projection to cut vetices into two sets.
	
	 *(in the presense of good balanced clustering, algo2. will only miscluster
	a few(bounded) rows.)},
  timestamp = {2012.04.05}
}

@ARTICLE{maila2001random,
  author = {Maila, M. and Shi, J.},
  title = {A Random Walks View of Spectral Segmentation},
  journal = {AI and STATISTICS (AISTATS) 2001},
  year = {2001},
  volume = {<<misssing>>},
  pages = {<<misssing>>},
  _h_pass1 = {2012.05.17},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {The random walk view of NCut, pointed by [von Luxburg 2007 tutorial].
	
	
	Main result: eq(6).
	
	 * The NCut induces the partition, s.t. a random walker seldom transition
	between clusters.},
  file = {:maila2001-A-Random-Walks-View-of-Spectral-Segmentation.pdf:PDF},
  owner = {hpl},
  timestamp = {2012.05.17}
}

@ARTICLE{nadler2007fundamental,
  author = {Nadler, B. and Galun, M.},
  title = {Fundamental limitations of spectral clustering},
  journal = {Advances in Neural Information Processing Systems},
  year = {2007},
  volume = {19},
  pages = {1017},
  _h_pass1 = {2012.04.09},
  _h_pass2 = {2012.04.09},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {Two spectral clustering manner:
	
	 * recursive cut using 2nd eigenvector. 
	
	 * embed points into first few eigenvectors and invoke traditional
	clustering algorithm.
	
	
	Address fundamental limitation incurred by conductance like criterion.
	
	
	Propose their own diffusion based notion.
	
	
	Proceedings:
	
	 * Diffusion process. 
	
	 * Stochastic differential equation.},
  file = {:nadler2007-Fundamental-limitations-of-spectral-clustering.pdf:PDF},
  owner = {hpl},
  publisher = {MIT; 1998},
  review = {Limitations according to examples:
	
	 * Irregular shapes. 
	
	 * Sensitive to density.(either similarity graph is constructed by
	kNN or Gaussian Kernel)
	
	
	Remarks:
	
	 * Yeas of time has passed and the community still recognizes conductance
	as one basic measure. 
	
	 * The fundamental limiation comes from the fact that similarity matrix
	boost local information and throw away global information. No matter
	Gaussian kernal or kNN is used, local information, or say topology
	is well reserved. But spatial strucuture is lost or blended then.},
  timestamp = {2012.04.05}
}

@ARTICLE{nadler2005diffusion,
  author = {Nadler, B. and Lafon, S. and Coifman, R.R. and Kevrekidis, I.G.},
  title = {Diffusion maps, spectral clustering and eigenfunctions of fokker-planck
	operators},
  journal = {Arxiv preprint math/0506090},
  year = {2005},
  volume = {<<missing>>},
  pages = {<<missing>>},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  file = {:nadler2005-Diffusion-maps-spectral-clustering-and-eigenfunctions-of-fokker-planck-operators.pdf:PDF},
  owner = {hpl},
  timestamp = {2012.02.12}
}

@ARTICLE{ng2002spectral,
  author = {Ng, A.Y. and Jordan, M.I. and Weiss, Y.},
  title = {On spectral clustering: Analysis and an algorithm},
  journal = {Advances in neural information processing systems},
  year = {2002},
  volume = {2},
  pages = {849--856},
  _h_pass1 = {2012.05.14},
  _h_pass2 = {2012.05.14},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {2k+ citation. Influential works.
	
	
	View spectral clustering as performing a kind of kernel PCA with a
	specific kernel.
	
	
	Analyze the problem using matrix perturbation theory.},
  file = {:ng2002-On-spectral-clustering-Analysis-and-an-algorithm.pdf:PDF},
  owner = {hpl},
  publisher = {MIT; 1998},
  review = {Process:
	
	 * Form affinity matrix by Gaussian ball. 
	
	 * Form normalized adjacency matrix(A). 
	
	 * Pick k principal eigenvectors of A: U. 
	
	 * Normalize each row of U: Y. (project onto unit sphere)
	
	 * K-means to cluster Y.
	
	
	Remarks:
	
	 * Since the points are already projected onto unit sphere, why not
	cluster using a modified version of K-means? (use angle instead of
	distance)
	
	 * They claim their Lsym is better than Lrw, the opposite of "von2007tutorial"},
  timestamp = {2012.02.11}
}

@ARTICLE{saerens2004principal,
  author = {Saerens, M. and Fouss, F. and Yen, L. and Dupont, P.},
  title = {The principal components analysis of a graph, and its relationships
	to spectral clustering},
  journal = {Machine Learning: ECML 2004},
  year = {2004},
  volume = {<<missing>>},
  pages = {371--383},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  file = {:saerens2004-The-principal-components-analysis-of-a-graph-and-its-relationships-to-spectral-clustering.pdf:PDF},
  owner = {hpl},
  publisher = {Springer},
  timestamp = {2012.02.12}
}

@ARTICLE{shi2000normalized,
  author = {Shi, J. and Malik, J.},
  title = {Normalized cuts and image segmentation},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year = {2000},
  volume = {22},
  pages = {888--905},
  number = {8},
  _h_pass1 = {2012.05.02},
  _h_pass2 = {2012.05.02},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {The journal version of [Shi and Malik 1997].
	
	[Shi and Malik 1997] is the Spectral Clustering prototype in computer
	vision. 
	
	
	Maybe the first to propose spectral clustering?
	
	
	Relax the NP combinatorial problem (NCut) to be polynomial time eigen
	decomposition problem.
	
	
	Quesionts:
	
	 * How to construct clusters from eigenvector of normalized Laplacian?
	(revursive 2 division?)(thresholding of the eigen vector)[SOLVED]},
  file = {:shi2000-Normalized-cuts-and-image-segmentation.pdf:PDF},
  owner = {hpl},
  publisher = {IEEE},
  review = {Method:(recursive 2-way)
	
	1. Construct similarity matrix. 
	
	2. Solve for left normalized Laplacian eigen decomposition. 
	
	3. Threshold the 2nd smallest eigen vector to cut graph into two.
	(the threshold which yields best NCut)
	
	4. Recurse on subgraphs. 
	
	
	Method:(k-way)
	
	1. Solve for more top eigenvectors. 
	
	2. Cluster in lower dimension. (like K-means) (divide into more parts
	than wanted)
	
	3. Post-processing. (greedy or cluster the condense graph)
	
	
	Other:
	
	 * Lanczos method of eigensolver, aiming at sparse matrix and when
	top eigen vectors are needed.
	
	 * NP-C proof of normalized cut on regular grids.},
  timestamp = {2012.05.02}
}

@INPROCEEDINGS{spielman1996spectral,
  author = {Spielman, DA and Teng, S.H.},
  title = {Spectral partitioning works: Planar graphs and finite element meshes},
  booktitle = {Foundations of Computer Science, 1996. Proceedings., 37th Annual
	Symposium on},
  year = {1996},
  pages = {96--105},
  organization = {IEEE},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {1},
  abstract = {from the cite of "guattery1998quality", this one is more close to
	Spectral Clustering works.},
  file = {:spielmat1996-Spectral-partitioning-works-Planar-graphs-and-finite-element-meshes.pdf:PDF},
  owner = {hpl},
  timestamp = {2012.02.12}
}

@ARTICLE{von2007tutorial,
  author = {Von Luxburg, U.},
  title = {A tutorial on spectral clustering},
  journal = {Statistics and Computing},
  year = {2007},
  volume = {17},
  pages = {395--416},
  number = {4},
  _h_pass1 = {2012.02.10},
  _h_pass2 = {2012.02.11},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {recommended from http://zhiyuan.sjtu.edu.cn/course/mathcs.htm
	
	
	Main steps of spectral clustering:
	
	1. Construct similarity matrix
	
	2. Compute Laplacian
	
	3. Normalize(optional)
	
	4. Eigen-decomposition. 
	
	5. Keep smallest k eigenvalues and corresponding eigenvectors(column).
	
	6. Use traditional clustering methods in reduce dimension(k-D).},
  file = {:vonluxburg2007-A-tutorial-on-spectral-clustering.pdf:PDF},
  owner = {hpl},
  publisher = {Springer},
  review = {Operational guides:
	
	1. Similarity measure is important and SC is unstable to changes of
	similarity measures. 
	
	2. Similarity graph is often built upon kNN (union version). For random
	graph, make k > log(n) as a practical rule. 
	
	3. Choice of spatial matrix: Lrw > Lsym > L, never Similarity, Weight
	matrix directly. Unnormalized version results in Dirac function(contains
	little information for clustering). 
	
	4. Choice of k(kNN for similarity graph): k > log(n). 
	
	5. Choice of epsilon: using max edge of MST. 
	
	6. Choice of sigma(for Gaussian kernel): similar rule as equivalent
	kNN or epsilon-graph. 
	
	7. Choice of k(number of clusters): the k-th smallest eigen value
	is much less than min{dj}
	
	
	Insights of spectral clustering:
	
	1. Graph cut. 
	
	2. Random walk / commute distance (on undirected graph)
	
	3. Perturbation
	
	
	Properties of clustering:
	
	1. Similarity sum is sparse between clusters. 
	
	2. Similarity sum is dens within cluster. 
	
	
	Graph cut:
	
	1. RatioCut(cardinality as denominator) --> Unnormalized L --> Don't
	carry 2nd property of clustering. 
	
	2. Ncut(volume as denominator) --> Normalized Lrw --> carry 2nd property
	of clustering
	
	3. Relax graph cut problem to trace minimization problem. 
	
	4. There are constructions to prove, SC cluster / optimal cluster
	is not bounded measured by RatioCut. 
	
	
	Random walk:
	
	1. Random walker is more likely to jump within cluster. 
	
	2. Random walker is not likely to jump between clusters. 
	
	3. (I - Lrw) corresponds to transition probability matrix of random
	walker. 
	
	4. Ncut <--> conditional probabilities of jumping between clusters.
	
	
	Commute distance:
	
	1. Close to SC+L, with eigen vectors scaled by the generalized inverse
	of L. 
	
	
	Perturbation theory:
	
	1. Difference of eigenvalues and eigenvectors are bounded by perturbation
	matrix. 
	
	2. Ideal separated connected components --> first k eigenvectors as
	cluster indicating vectors. 
	
	3. k connected components with sparse cuts --> idea separated connected
	components with small perturbation --> first k eigenvectors as perturbed
	indicating vectors --> k-means to "round" those vectors back to ideal
	indicating vector. 
	
	
	My question:
	
	 * It seems spectral algorithms I see so far all operates on undirected
	graphs. However, algorithms like PR are applied to directed context
	long before. How do the counterparts of those beautiful results of
	undirected graph look like in the context of directed graphs.
	
	
	Future:
	
	1. Fig4(3) can be improved. I'll try "common neighbours"(upon epsilon
	graph) as the similarity measure, which should capture density-clusters
	rather than perfect boundary clusters.},
  timestamp = {2012.01.13}
}

@ARTICLE{von2008consistency,
  author = {Von Luxburg, U. and Belkin, M. and Bousquet, O.},
  title = {Consistency of spectral clustering},
  journal = {The Annals of Statistics},
  year = {2008},
  volume = {36},
  pages = {555--586},
  number = {2},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  file = {:vonluxburg2008-Consistency-of-spectral-clustering.pdf:PDF},
  owner = {hpl},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2012.02.12}
}

@ARTICLE{von2005limits,
  author = {Von Luxburg, U. and Bousquet, O. and Belkin, M.},
  title = {Limits of spectral clustering},
  journal = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2005},
  volume = {17},
  pages = {857--864},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  file = {:vonluxburg2005-Limits-of-spectral-clustering.pdf:PDF},
  owner = {hpl},
  publisher = {Citeseer},
  timestamp = {2012.02.12}
}

