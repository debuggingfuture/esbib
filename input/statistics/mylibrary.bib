% This file was created with JabRef 2.7b.
% Encoding: UTF-8

@BOOK{berger1985statistical,
  title = {Statistical decision theory and Bayesian analysis},
  publisher = {Springer},
  year = {1985},
  author = {Berger, J.O.},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {Early works of decision theory.
	
	
	Principles like, minimal misclassification error, etc.},
  file = {:berger1985-Statistical-decision-theory-and-Bayesian-analysis.txt:Text},
  owner = {hpl},
  timestamp = {2012.03.12}
}

@ARTICLE{dempster1977em,
  author = {Dempster, A.P. and Laird, N.M. and Rubin, D.B.},
  title = {Maximum likelihood from incomplete data via the EM algorithm},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  year = {1977},
  volume = {39},
  pages = {1--38},
  _h_pass1 = {2011.10.04},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {The inventor of EM method, a citation 20k+!
	
	
	EM, MCEM, Data Augmentation, Genetic Linkage Model
	
	
	PRECEDINGS:
	
	1. Jensen inequality
	
	2. Real Analysis
	
	3. PMDA(Poor Man Data Augmentation)},
  file = {:dempster1977-Maximum-likelihood-from-incomplete-data-via-the-EM-algorithm.pdf:PDF},
  owner = {hpl},
  publisher = {JSTOR},
  timestamp = {2011.10.04}
}

@ARTICLE{van2001art-data-augmentation,
  author = {van Dyk, D.A. and Meng, X.L.},
  title = {The art of data augmentation},
  journal = {Journal of Computational and Graphical Statistics},
  year = {2001},
  volume = {10},
  pages = {1--50},
  number = {1},
  _h_pass1 = {2011.10.04},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  abstract = {Category:
	
	1. Deterministic, like EM, 
	
	2. Stochastic algorithm, Data Augmentation, 
	
	3. phisical --> auxiliary variables
	
	4. make simulation simple and reasonable.
	
	
	Keywords:
	
	 Auxiliaryvariables;Conditionalaugmentation;EM algorithm;Gibbssampler;
	
	Haar measure; Hierarchical models; Marginal augmentation; Markov chain
	Monte Carlo;
	
	Mixed-effectsmodels; Nonpositive recurrent Markov chain; Posterior
	distributions;Probit
	
	regression; Rate of convergence},
  file = {:van2001-The-art-of-data-augmentation.pdf:PDF},
  owner = {hpl},
  publisher = {ASA},
  timestamp = {2011.10.04}
}

@BOOK{lehmann1998theory,
  title = {Theory of point estimation},
  publisher = {Springer Verlag},
  year = {1998},
  author = {Lehmann, E.L. and Casella, G.},
  volume = {31},
  _h_pass1 = {2012.03.12},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {140-143, Deduction of Fisher Information Equality.},
  file = {:lehmann1998-Theory-of-point-estimation.pdf:PDF},
  owner = {hpl},
  timestamp = {2012.03.12},
  url = {http://www.yu.ac.ir/DesktopModules/Contents/assets/asset2735/Theory_of_Point_Estimation_Cas_Leh_2nd.pdf}
}

@BOOK{mclachlan1997algorithm,
  title = {The EM algorithm and extensions},
  publisher = {Wiley New York},
  year = {1997},
  author = {McLachlan, G.J. and Krishnan, T.},
  volume = {274},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {The book contains convergence issue of EM.
	
	
	The Q function does increase in each round and can reach some stationary
	point. 
	
	The stationary point can be either local maxima or saddle point.
	
	
	Generalized EM:
	
	 * Modify the M-step. No longer need to maximize the conditional expected
	joint likelihood. Just make it increasing.
	
	 * Same analysis as for EM. 
	
	 * Apply to the situation where Maximization can not be done explicitly.
	
	
	EM algorithm has special advantage over exponential families.},
  file = {:mclachlan1997-The-EM-algorithm-and-extensions.txt:Text},
  owner = {hpl},
  timestamp = {2012.04.15}
}

@ARTICLE{meng1993maximum,
  author = {Meng, X.L. and Rubin, D.B.},
  title = {Maximum likelihood estimation via the ECM algorithm: A general framework},
  journal = {Biometrika},
  year = {1993},
  volume = {80},
  pages = {267--278},
  number = {2},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  abstract = {One of Generalized EM algorithm. 
	
	
	In M-step, make several constrained optimization.},
  file = {:meng1993-Maximum-likelihood-estimation-via-the-ECM-algorithm-A-general-framework.pdf:PDF},
  owner = {hpl},
  publisher = {Biometrika Trust},
  timestamp = {2012.03.29}
}

@INPROCEEDINGS{neal1993anew,
  author = {Neal, Radford M. and Hinton, Geoffrey E.},
  title = {A New View of the EM Algorithm that Justifies Incremental and Other
	Variants},
  booktitle = {Learning in Graphical Models},
  year = {1993},
  pages = {355--368},
  publisher = {Kluwer Academic Publishers},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  abstract = {Modify the E-step rather than computing for a complete marginalization.
	
	
	Incremental modifications to E-step. If the components are of exponential
	family. Responsibility( p(class|sample) ) enters through simple sufficent
	statistics. Incremental updated for M-steps is possible then.
	
	
	Empirical results show this incremental approach converges faster
	than batch mode.},
  file = {:neal1993-A-New-View-of-the-EM-Algorithm-that-Justifies-Incremental-and-Other-Variants.pdf:PDF},
  owner = {hpl},
  timestamp = {2012.03.29}
}

@ARTICLE{o1986statistical,
  author = {O'Sullivan, F.},
  title = {A statistical perspective on ill-posed inverse problems},
  journal = {Statistical Science},
  year = {1986},
  volume = {1},
  pages = {502--518},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  abstract = {Network tomography is a kind of ill-posed problem, this paper may
	provide me with a mathematical view of the whole category of problem,
	regardless its publication date.},
  file = {:sullivan1986-A-statistical-perspective-on-ill-posed-inverse-problems.pdf:PDF},
  owner = {hpl},
  publisher = {JSTOR},
  timestamp = {2011.09.29}
}

@ARTICLE{robbins1951stochastic,
  author = {Robbins, H. and Monro, S.},
  title = {A stochastic approximation method},
  journal = {The Annals of Mathematical Statistics},
  year = {1951},
  volume = {<<missing>>},
  pages = {400--407},
  _h_pass1 = {2012.03.12},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {1},
  abstract = {The famous Robbins-Monro method. 
	
	
	M(x): a monotonic function of x. 
	
	Want to find the solution of M(x)=?? by experiments.
	
	
	Outline:(concluded from Bishop's PRML)
	
	
	Essence:
	
	 * Like Gradient Descent, but design a proper step size. 
	
	 * Step sizes are chosen according to a series. {a}
	
	
	Series {a}:
	
	 * Limit of {a} = 0 
	
	 * Summation of {a} = infinite (divergent)
	
	 * Squared summation, {a^2} = finite
	
	(the harmonic series satisfy the three criteria very well!)},
  file = {:robbins1951-A-stochastic-approximation-method.pdf:PDF},
  owner = {hpl},
  publisher = {JSTOR},
  timestamp = {2012.03.12}
}

@BOOK{tarantola1987inverse,
  title = {Inverse problem theory},
  publisher = {SIAM},
  year = {2004},
  author = {Tarantola, A. and Tarantola, A.},
  volume = {130},
  _h_pass1 = {undefined},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {-1},
  file = {:http\://www.ipgp.fr/~tarantola/Files/Professional/SIAM/index.html:URL},
  owner = {hpl},
  timestamp = {2011.10.11}
}

@ARTICLE{vitter1985-random-reservoir,
  author = {Vitter, J.S.},
  title = {Random sampling with a reservoir},
  journal = {ACM Transactions on Mathematical Software (TOMS)},
  year = {1985},
  volume = {11},
  pages = {37--57},
  number = {1},
  _h_pass1 = {2011.09.27},
  _h_pass2 = {2011.09.27},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {Applied for stream sampling, where you do not know the size of the
	full set.},
  file = {:vitter1985-Random-sampling-with-a-reservoir.pdf:PDF},
  owner = {hpl},
  publisher = {ACM},
  review = {Sample n from N problem 
	
	reservoir is sise n
	
	
	CONCEPT1:
	
	1. maintain a list of size n 
	
	2. scan through N records
	
	3. determine if current record should be put in reservoir.
	
	4. determine which record in reservoir to be substituted, if necessary.
	
	
	(keep the probability of every element in reservoir to be n/t, where
	t is the number of processed records)
	
	
	CONCEPT2:
	
	1. using skip to substitute generating random variable for every records.
	
	2. the problem is to figure out how to generate psai(n,t) <-- the
	next record to be put into reservoir
	
	
	CONTRIBUTION:
	
	1. Deduce a lower bound for reservoir algorithm, claim the Z method
	to be optimum. 
	
	
	LIMIT:
	
	1. n must be determined beforehand. What is the solution when we only
	know the expected ratio of n/N?},
  timestamp = {2011.09.27}
}

@ARTICLE{vitter1984-faster-random-sampling,
  author = {Vitter, J.S.},
  title = {Faster methods for random sampling},
  journal = {Communications of the ACM},
  year = {1984},
  volume = {27},
  pages = {703--718},
  number = {7},
  _h_pass1 = {2011.09.27},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {an online algorithm. provide O(n) performance, without preprocessing.
	constant space.
	
	
	This paper is only here for further use.},
  file = {:vitter1984-Faster-methods-for-random-sampling.pdf:PDF},
  owner = {hpl},
  publisher = {ACM},
  timestamp = {2011.09.27}
}

@ARTICLE{wei1990monte-carlo-em,
  author = {Wei, G.C.G. and Tanner, M.A.},
  title = {A Monte Carlo implementation of the EM algorithm and the poor man's
	data augmentation algorithms},
  journal = {Journal of the American Statistical Association},
  year = {1990},
  volume = {85},
  pages = {699--704},
  _h_pass1 = {2011.10.04},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {0},
  abstract = {Monte Carlo implementation of the E-step.
	
	
	Data augmentation.},
  file = {:wei1990-A-Monte-Carlo-implementation-of-the-EM-algorithm-and-the-poor-man-data-augmentation-algorithms.pdf:PDF},
  owner = {hpl},
  publisher = {JSTOR},
  timestamp = {2011.09.26}
}

@MISC{wiki-expectation-maximum,
  author = {Wikipedia},
  title = {Expectation Maximum},
  _h_pass1 = {2011.09.26},
  _h_pass2 = {undefined},
  _h_pass3 = {undefined},
  _h_priority = {0},
  file = {:http\://en.wikipedia.org/wiki/Expectation-maximization_algorithm:URL},
  owner = {hpl},
  timestamp = {2011.09.26}
}

